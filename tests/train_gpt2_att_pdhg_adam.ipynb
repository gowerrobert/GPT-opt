{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47534842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import contextlib\n",
    "from torch.optim import Optimizer\n",
    "from gptopt.train import train\n",
    "from gptopt.optim.utils import get_scheduler, get_optimizer\n",
    "from gptopt.utils import hash_config, set_seed, get_worker_info\n",
    "from gptopt.model import load_model\n",
    "from gptopt.data.data_utils import get_data_dir\n",
    "from gptopt.dataloader import ShardedDataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import wandb\n",
    "import hydra\n",
    "import time\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from gptopt.train import Logging, eval_validation_loss\n",
    "from gptopt.gpt_model import CausalSelfAttention\n",
    "from gptopt.utils import get_worker_info, save_checkpoint, load_checkpoint\n",
    "\n",
    "OmegaConf.register_new_resolver(\"div\", lambda x, y: x // y)\n",
    "\n",
    "typedict = {\"float16\":torch.float16, \"float32\":torch.float32, \"bfloat16\":torch.bfloat16}\n",
    "from pdhg import pdhg_method_AB, prox_l1, AttnPDHGAdamW\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf60db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, model, optimizer, training_params, logging_params, scheduler=None, ckpt_dir=\"\", wandb_run=None):\n",
    "    record_pdhg_info = [0, 10, 100]\n",
    "    world_size, rank, local_rank, device  = get_worker_info()\n",
    "    master_process = (rank == 0)\n",
    "    logger = Logging()\n",
    "    logger.pdhg_residuals = []\n",
    "    optimizer_name = optimizer.__class__.__name__\n",
    "    if 'momo' in optimizer_name.lower() or 'nesgd' in optimizer_name.lower():\n",
    "        pass_loss = True\n",
    "    else:\n",
    "        pass_loss = False\n",
    "    if master_process: print(f\"Set pass_loss to {pass_loss} for optimizer {optimizer_name}\")\n",
    "\n",
    "    autocast_ctxt = contextlib.nullcontext()\n",
    "    if training_params['autocast']:\n",
    "        autocast_ctxt = torch.autocast(device_type=device, dtype=typedict[training_params['mixed_precision']])     \n",
    "    B, T = training_params['batch_size'], training_params['context_length']\n",
    "    grad_accum_steps = int(training_params['tokens_processed'] / (world_size*B*T))\n",
    "    val_accum_steps = int(logging_params['val_tokens_processed'] / (world_size*B*T))\n",
    "    if master_process: print(f\"Accumulate gradient for {grad_accum_steps} steps\")\n",
    "    total_iterations = int(training_params['num_epochs'] * len(train_dataloader) / training_params['tokens_processed'])\n",
    "    max_grad_norm = training_params['gradnorm'] if training_params['gradnorm'] != 0. else float('inf')\n",
    "\n",
    "    load_ckpt_step = logging_params['load_ckpt_step']\n",
    "    if load_ckpt_step != 0:\n",
    "        model, optimizer, train_dataloader, scheduler = load_checkpoint(ckpt_dir, load_ckpt_step, model, \\\n",
    "                                                        optimizer, train_dataloader, scheduler=None)\n",
    "    if ckpt_dir == \"\":\n",
    "        print(\"Will not save checkpoints as no directory is specified\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(training_params['num_epochs']):\n",
    "        if master_process:\n",
    "            print(f\"Epoch {epoch+1} of {training_params['num_epochs']}\")\n",
    "\n",
    "        model.train()\n",
    "        start_epoch = time.time()\n",
    "        start_time = time.time() \n",
    "        loss_accum = 0.\n",
    "        step = 1 if load_ckpt_step == 0 else int(load_ckpt_step)  # micro-step counter\n",
    "        opt_step = 0 if load_ckpt_step == 0 else int(load_ckpt_step) // grad_accum_steps  # optimizer step counter\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if step != 1 and master_process:\n",
    "            print(train_dataloader.get_state())\n",
    "            print(f\"Resuming from micro_step={step}, opt_step={opt_step}\")\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            with autocast_ctxt:\n",
    "                output = model(input_ids=batch[0], labels=batch[1])\n",
    "                loss = (output.loss if hasattr(output, \"loss\") else output[1])\n",
    "                loss /= grad_accum_steps\n",
    "\n",
    "            loss_accum += loss.detach()\n",
    "                \n",
    "            # Check if accummulated enough gradients to take a step\n",
    "            if step % grad_accum_steps != 0:\n",
    "                with (model.no_sync() if world_size > 1 else contextlib.nullcontext()):\n",
    "                    loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                if world_size > 1: dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "                if pass_loss:\n",
    "                    _, pdhg_residuals = optimizer.step(closure=None, loss=loss_accum)\n",
    "                else:\n",
    "                    _, pdhg_residuals = optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "                if step % grad_accum_steps in record_pdhg_info:\n",
    "                    logger.pdhg_residuals.append(pdhg_residuals)\n",
    "                    \n",
    "                #bookkeeping\n",
    "                torch.cuda.synchronize()\n",
    "                step_time = time.time() - start_time\n",
    "                # Count an optimizer step\n",
    "                opt_step += 1\n",
    "                if master_process and wandb_run is not None:\n",
    "                    wandb_log_dict = {\n",
    "                        \"train/loss\": loss_accum.item(), \n",
    "                        \"train/grad_norm\": norm.item(),\n",
    "                        \"train/step_time\": step_time,\n",
    "                        \"train/step\": opt_step,\n",
    "                        \"train/micro_step\": step\n",
    "                    }\n",
    "                    if getattr(model.config, \"record_kq_max\", False):\n",
    "                        base_model = getattr(model, \"module\", model)\n",
    "                        kq_max = None\n",
    "                        for m in base_model.modules():\n",
    "                            if isinstance(m, CausalSelfAttention) and getattr(m, \"kq_max\", None) is not None:\n",
    "                                v = m.kq_max\n",
    "                                if kq_max is None or v > kq_max:\n",
    "                                    kq_max = v\n",
    "                        if kq_max is not None:\n",
    "                            wandb_log_dict[\"train/kq_max\"] = kq_max\n",
    "                            logger.kq_max.append(kq_max)\n",
    "                    if hasattr(optimizer, 'step_size_list'):\n",
    "                        wandb_log_dict[\"train/step_size_list\"] = optimizer.step_size_list\n",
    "                    for param_group_ix, param_group in enumerate(optimizer.param_groups):\n",
    "                        wandb_log_dict[f\"train/lr_{param_group_ix}\"] = param_group['lr']\n",
    "                    wandb_run.log(wandb_log_dict)\n",
    "                logger.step_times.append(step_time)  # Are these different across ranks?\n",
    "                logger.grad_norms.append(norm.item())\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    logger.learning_rates.append(param_group['lr'])\n",
    "                logger.losses.append(loss_accum.item())\n",
    "                if hasattr(optimizer, 'step_size_list'):  \n",
    "                    logger.step_size_list = optimizer.step_size_list  \n",
    "                \n",
    "                if (opt_step % logging_params['log_step'] == 0) & master_process:\n",
    "                    tps = training_params[\"tokens_processed\"] / step_time\n",
    "                    print(f\"Step {opt_step} of {total_iterations} (optimizer steps).\")\n",
    "                    print(f\"Time taken : {step_time*1000:0.1f}ms | Tokens/s : {tps/1000:0.1f}k | Loss : {loss_accum.item():0.3f} | Accum: {grad_accum_steps} micro-steps/opt-step\")\n",
    "                    \n",
    "                if (opt_step % logging_params['val_step'] == 0):\n",
    "                    val_loss = eval_validation_loss(model, val_dataloader, val_accum_steps, autocast_ctxt)\n",
    "                    if master_process and wandb_run is not None:\n",
    "                        wandb_run.log({\"val/loss\": val_loss.item(), \"val/step\": opt_step, \"val/micro_step\": step})\n",
    "                    logger.val_losses.append(val_loss.item())\n",
    "\n",
    "                if (opt_step % logging_params['save_ckpt_step'] == 0) & (ckpt_dir != \"\"):\n",
    "                    save_checkpoint(ckpt_dir, step, model, optimizer, loss_accum.item(),\n",
    "                                    train_dataloader, scheduler, logging_params['keep_last'])\n",
    "                    \n",
    "                    if master_process:\n",
    "                        with open(ckpt_dir + '/log.json', 'w') as file:\n",
    "                            json.dump(logger.__dict__, file)\n",
    "                loss_accum = 0.\n",
    "                start_time = time.time() \n",
    "            step += 1\n",
    "            \n",
    "            \n",
    "        print(f\"In rank: {rank}, epoch {epoch+1}, Train Loss: {logger.losses[-1]}\")\n",
    "        print(f\"In rank: {rank}, time taken for epoch {epoch+1} : \", time.time() - start_epoch)\n",
    "        \n",
    "        # Evaluate on val set, and save final values\n",
    "        val_dataloader.reset()\n",
    "        val_loss = eval_validation_loss(model, val_dataloader, 0, autocast_ctxt)\n",
    "        logger.val_losses.append(val_loss.item())\n",
    "        print(f\"In rank: {rank}, epoch {epoch+1}, Validation Loss: {val_loss.item()}\") \n",
    "        if getattr(model.config, \"record_kq_max\", False):\n",
    "            base_model = getattr(model, \"module\", model)\n",
    "            kq_max = None\n",
    "            for m in base_model.modules():\n",
    "                if isinstance(m, CausalSelfAttention) and getattr(m, \"kq_max\", None) is not None:\n",
    "                    v = m.kq_max\n",
    "                    if kq_max is None or v > kq_max:\n",
    "                        kq_max = v  \n",
    "                        logger.val_kq_max.append(kq_max)     \n",
    "        if (ckpt_dir != \"\"):\n",
    "            save_checkpoint(ckpt_dir, step, model, optimizer, logger.losses[-1],\n",
    "                        train_dataloader, scheduler, logging_params['keep_last'])        \n",
    "            if master_process:\n",
    "                with open(ckpt_dir + '/log.json', 'w') as file:\n",
    "                    json.dump(logger.__dict__, file)\n",
    "        if master_process and wandb_run is not None:\n",
    "            wandb_log_dict = {\n",
    "                \"val/loss\": val_loss.item(),\n",
    "                \"val/step\": opt_step,\n",
    "                \"val/micro_step\": step,\n",
    "                \"train/loss\": logger.losses[-1],\n",
    "                \"train/step\": opt_step,\n",
    "                \"train/micro_step\": step,\n",
    "            }\n",
    "            if getattr(model.config, \"record_kq_max\", False) and kq_max is not None:\n",
    "                    wandb_log_dict[\"val/kq_max\"] = kq_max\n",
    "            wandb_run.log(wandb_log_dict)\n",
    "\n",
    "    if hasattr(optimizer, 'step_size_list'):      # Check if optimizer has a step_size_list attribute\n",
    "        logger.step_size_list = optimizer.step_size_list  \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103c8263",
   "metadata": {},
   "outputs": [],
   "source": [
    "@hydra.main(version_base=None, config_path=\"/mnt/home/tparshakova/Documents/GPT-opt/hydra_conf\", config_name=\"config\")\n",
    "def main(config: DictConfig):\n",
    "    set_seed(42)\n",
    "\n",
    "    # Establish Hydra run directory for saving outputs\n",
    "    hydra_run_dir = HydraConfig.get().runtime.output_dir\n",
    "    os.makedirs(hydra_run_dir, exist_ok=True)\n",
    "\n",
    "    # First set up DDP\n",
    "    ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?\n",
    "    if ddp:\n",
    "        # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
    "        assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
    "        dist.init_process_group(backend=\"nccl\")\n",
    "    world_size, rank, local_rank, device = get_worker_info()\n",
    "    master_process = rank == 0  # this process will do logging, checkpointing etc.\n",
    "    device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Set the training parameters\n",
    "    training_params = config[\"training\"][\"training_params\"]\n",
    "    opt_config = config[\"optimizer\"][\"optimizer_params\"]\n",
    "    logging_config = config[\"logging\"][\"logging_params\"]\n",
    "    model_config = config[\"model\"][\"config\"]\n",
    "    model_name = config[\"model\"][\"name\"]\n",
    "    # Logging\n",
    "    outputname = HydraConfig.get().job.config_name\n",
    "    # Save results into Hydra's run directory for this job\n",
    "    logging_config[\"results_dir\"] = hydra_run_dir\n",
    "    output_dir = hydra_run_dir\n",
    "\n",
    "    CKPT_DIR = logging_config[\"ckpt_dir\"]\n",
    "    ckpt_dir_base = f\"{CKPT_DIR}/{outputname}/\" if CKPT_DIR != \"\" else \"\"\n",
    "    if master_process:\n",
    "        print(f\"Training on dataset {config['data']['dataset']['name']}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        if CKPT_DIR != \"\":\n",
    "            os.makedirs(ckpt_dir_base, exist_ok=True)\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(model_name, model_config, device)\n",
    "    torch.set_float32_matmul_precision(training_params[\"tensorcore_precision\"])\n",
    "\n",
    "    # Load data\n",
    "    data_dir = get_data_dir(config[\"data\"][\"dataset\"][\"name\"])\n",
    "    dataset_path = data_dir + f\"/{config['data']['dataset']['name']}-gpt2/\"\n",
    "    if master_process:\n",
    "        print(f\"Load data from {dataset_path}\")\n",
    "    B, T = training_params[\"batch_size\"], training_params[\"context_length\"]\n",
    "    assert training_params[\"tokens_processed\"] % (world_size * B * T) == 0\n",
    "    num_microbatches = int(\n",
    "        training_params[\"tokens_processed\"] / (world_size * B * T)\n",
    "    )\n",
    "\n",
    "    train_dataloader = ShardedDataLoader(dataset_path, B, T, \"train\", device)\n",
    "    val_dataloader = ShardedDataLoader(dataset_path, B, T, \"val\", device)\n",
    "    total_iterations = int(\n",
    "        training_params[\"num_epochs\"]\n",
    "        * len(train_dataloader)\n",
    "        / training_params[\"tokens_processed\"]\n",
    "    )\n",
    "    if master_process:\n",
    "        print(\n",
    "            f\"Length of train dataset : {len(train_dataloader)/1e6:0.1f} million tokens\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Length of validation dataset : {len(val_dataloader)/1e6:0.1f} million tokens\"\n",
    "        )\n",
    "        print(f\"Total number of iterations : {total_iterations}\")\n",
    "\n",
    "    print()\n",
    "    if master_process:\n",
    "        print(\n",
    "            f\"Training with optimizer {opt_config['name']} and learning rate {opt_config['lr']}\"\n",
    "        )\n",
    "\n",
    "    # Generate hash for the current optimizer configuration\n",
    "    config_hash = hash_config(\n",
    "        OmegaConf.to_container(opt_config),\n",
    "        OmegaConf.to_container(training_params),\n",
    "        OmegaConf.to_container(model_config),\n",
    "    )\n",
    "    file_name = (\n",
    "        f\"{opt_config['name']}-lr-{opt_config['lr']}-{opt_config['lr_schedule']}\"\n",
    "    )\n",
    "    if \"muon_lr\" in opt_config:\n",
    "        file_name += f\"-muonlr-{opt_config['muon_lr']}\"\n",
    "    file_name += f\"-{config_hash}\"\n",
    "    output_path = os.path.join(output_dir, file_name + \".json\")\n",
    "    ckpt_dir = (\n",
    "        os.path.join(ckpt_dir_base, file_name) + \"/\" if CKPT_DIR != \"\" else \"\"\n",
    "    )\n",
    "\n",
    "    # copy model to ensure consistency\n",
    "    model_copy = copy.deepcopy(model).to(device)\n",
    "    opt_name = opt_config[\"name\"]\n",
    "    # Setup optimizer: allow using local AttnPDHGAdamW in this test\n",
    "    if opt_name == \"attn_pdhg_adamw\":\n",
    "        lr = float(opt_config.get(\"lr\", 1e-3))\n",
    "        betas = tuple(opt_config.get(\"betas\", (0.9, 0.999)))\n",
    "        eps = float(opt_config.get(\"eps\", 1e-8))\n",
    "        weight_decay = float(opt_config.get(\"weight_decay\", 0.01))\n",
    "        qk_lr_scale = float(opt_config.get(\"qk_lr_scale\", 1.0))\n",
    "        max_norm_tr = float(opt_config.get(\"max_norm_tr\", 1.0))\n",
    "        optimizer = AttnPDHGAdamW(\n",
    "            model_copy.named_parameters(),\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            qk_lr_scale=qk_lr_scale,\n",
    "            max_norm_tr=max_norm_tr,\n",
    "        )\n",
    "        use_my_adamw = True\n",
    "    else:\n",
    "        optimizer_obj, hyperp = get_optimizer(opt_config, lr=opt_config[\"lr\"])\n",
    "        use_my_adamw = False\n",
    "\n",
    "    if training_params[\"compile\"]:\n",
    "        if master_process:\n",
    "            print(\"Compiling model\")\n",
    "        model_copy = torch.compile(model_copy)\n",
    "\n",
    "    if ddp:\n",
    "        model_copy = DDP(model_copy, device_ids=[local_rank])\n",
    "\n",
    "    if not use_my_adamw:\n",
    "        p = (\n",
    "            model_copy.named_parameters()\n",
    "            if (\"muon\" in opt_name or \"scion\" in opt_name)\n",
    "            else model_copy.parameters()\n",
    "        )\n",
    "        optimizer = optimizer_obj(p, **hyperp)\n",
    "\n",
    "    scheduler = get_scheduler(\n",
    "        opt_config, optimizer, total_iterations=total_iterations\n",
    "    )\n",
    "\n",
    "    # Initialize wandb\n",
    "    if master_process and logging_config.get(\"wandb\", None) is not None:\n",
    "        config_no_optimizer = copy.deepcopy(config)\n",
    "        config_no_optimizer = OmegaConf.to_container(\n",
    "            config_no_optimizer, resolve=True\n",
    "        )\n",
    "        config_no_optimizer.pop(\"optimizer_params\", None)\n",
    "\n",
    "        wandb_config = dict(\n",
    "            one_optimizer_params=opt_config,\n",
    "            **config_no_optimizer,\n",
    "            world_size=world_size,\n",
    "        )\n",
    "\n",
    "        wandb_args = dict(logging_config[\"wandb\"])\n",
    "        wandb_args.setdefault(\n",
    "            \"dir\", f\"{logging_config['results_dir']}/../wandb\"\n",
    "        )\n",
    "\n",
    "        wandb_run = wandb.init(\n",
    "            **wandb_args,\n",
    "            config=wandb_config,\n",
    "            reinit=\"create_new\",\n",
    "        )\n",
    "    else:\n",
    "        wandb_run = None\n",
    "\n",
    "    # Train\n",
    "    try:\n",
    "        logger = train(\n",
    "            train_dataloader,\n",
    "            val_dataloader,\n",
    "            model_copy,\n",
    "            optimizer,\n",
    "            training_params,\n",
    "            scheduler=scheduler,\n",
    "            ckpt_dir=ckpt_dir,\n",
    "            logging_params=logging_config,\n",
    "            wandb_run=wandb_run,\n",
    "        )\n",
    "    finally:\n",
    "        if master_process and wandb_run is not None:\n",
    "            wandb_run.finish()\n",
    "\n",
    "    # Save\n",
    "    if master_process:\n",
    "        logger.name = opt_config[\"name\"] + \"-lr-\" + str(opt_config[\"lr\"])\n",
    "        if \"muon_lr\" in opt_config:\n",
    "            logger.name += f\"-muonlr-{opt_config['muon_lr']}\"\n",
    "        if \"muon_lr_ratio\" in opt_config:\n",
    "            logger.name += f\"-muonlr_ratio-{opt_config['muon_lr_ratio']}\"\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"File {output_path} already exists. Overwriting\")\n",
    "        with open(output_path, \"w\") as file:\n",
    "            json.dump(logger.__dict__, file)\n",
    "        print(f\"Saved output to {output_path}\")\n",
    "\n",
    "    if ddp:\n",
    "        dist.destroy_process_group()\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f30c10f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training on dataset tiny_shakespeare\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from /mnt/ceph/users/cmodi/huggingface/tiny_shakespeare-gpt2/\n",
      "Initialized train dataloader in 0 at :  {'rank': 0, 'position': 0, 'shard': 0}\n",
      "Initialized val dataloader in 0 at :  {'rank': 0, 'position': 0, 'shard': 0}\n",
      "Length of train dataset : 0.3 million tokens\n",
      "Length of validation dataset : 0.0 million tokens\n",
      "Total number of iterations : 147\n",
      "\n",
      "Training with optimizer attn_pdhg_adamw and learning rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtparshakova\u001b[0m (\u001b[33mtparshakova-simons-foundation\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/home/tparshakova/Documents/GPT-opt/tests/outputs/gpt-tiny/default/tiny_shakespeare/attn_pdhg_adamw/bs-4-lr-0.001-wd-0/../wandb/wandb/run-20251122_212226-bwwmzh75</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tparshakova-simons-foundation/gpt-opt/runs/bwwmzh75' target=\"_blank\">gpt-tiny_default_tiny_shakespeare_attn_pdhg_adamw_bs-4-lr-0.001-wd-0</a></strong> to <a href='https://wandb.ai/tparshakova-simons-foundation/gpt-opt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tparshakova-simons-foundation/gpt-opt' target=\"_blank\">https://wandb.ai/tparshakova-simons-foundation/gpt-opt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tparshakova-simons-foundation/gpt-opt/runs/bwwmzh75' target=\"_blank\">https://wandb.ai/tparshakova-simons-foundation/gpt-opt/runs/bwwmzh75</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set pass_loss to False for optimizer AttnPDHGAdamW\n",
      "Accumulate gradient for 2 steps\n",
      "Will not save checkpoints as no directory is specified\n",
      "Epoch 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Problem finishing run\n",
      "Error executing job with overrides: ['model=gpt-tiny', 'optimizer=attn_pdhg_adamw', 'data=shakespeare', 'training=shakespeare']\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1179253/1332419913.py\", line 168, in main\n",
      "    logger = train(\n",
      "             ^^^^^^\n",
      "  File \"/tmp/ipykernel_1179253/486920329.py\", line 66, in train\n",
      "    _, pdhg_residuals = optimizer.step()\n",
      "                        ^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/torch/optim/lr_scheduler.py\", line 133, in wrapper\n",
      "    return func.__get__(opt, opt.__class__)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/torch/optim/optimizer.py\", line 517, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/tests/pdhg.py\", line 101, in step\n",
      "    Z1_t, Z2_t, residuals, norm_Y = pdhg_method_AB(\n",
      "                                    ^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/tests/pdhg.py\", line 251, in pdhg_method_AB\n",
      "    Y = Y_new\n",
      "    ^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1179253/1332419913.py\", line 181, in main\n",
      "    wandb_run.finish()\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/wandb_run.py\", line 397, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/wandb_run.py\", line 442, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/wandb_run.py\", line 2260, in finish\n",
      "    return self._finish(exit_code)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/wandb_run.py\", line 397, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/wandb_run.py\", line 2288, in _finish\n",
      "    self._atexit_cleanup(exit_code=exit_code)\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/wandb_run.py\", line 2483, in _atexit_cleanup\n",
      "    self._on_finish()\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/wandb_run.py\", line 2693, in _on_finish\n",
      "    exit_handle = self._backend.interface.deliver_exit(self._exit_code)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/interface/interface.py\", line 1053, in deliver_exit\n",
      "    return self._deliver_exit(exit_data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/interface/interface_shared.py\", line 460, in _deliver_exit\n",
      "    return self._deliver(record)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/interface/interface_sock.py\", line 50, in _deliver\n",
      "    return self._asyncer.run(lambda: self.deliver_async(record))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py\", line 136, in run\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.11/concurrent/futures/_base.py\", line 456, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib64/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/lib/asyncio_manager.py\", line 219, in _wrap\n",
      "    return await fn()\n",
      "           ^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/interface/interface_sock.py\", line 58, in deliver_async\n",
      "    handle = await self._client.deliver(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/lib/service/service_client.py\", line 53, in deliver\n",
      "    handle = self._mailbox.require_response(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/wandb/sdk/mailbox/mailbox.py\", line 76, in require_response\n",
      "    raise MailboxClosedError()\n",
      "wandb.sdk.mailbox.mailbox.MailboxClosedError\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Optionally: provide overrides like on the CLI\n",
    "overrides = [\n",
    "    \"model=gpt-tiny\",\n",
    "    \"optimizer=attn_pdhg_adamw\",\n",
    "    \"data=shakespeare\",\n",
    "    \"training=shakespeare\",\n",
    "]\n",
    "\n",
    "# Hydra’s Launcher will use sys.argv, so we simulate a CLI:\n",
    "import sys\n",
    "old_argv = sys.argv\n",
    "try:\n",
    "    sys.argv = [old_argv[0]] + overrides\n",
    "    logger = main()  # this calls the @hydra.main-wrapped function\n",
    "finally:\n",
    "    sys.argv = old_argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7821235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from types import SimpleNamespace\n",
    "\n",
    "with open(\"outputs/gpt-tiny/default/tiny_shakespeare/attn_pdhg_adamw/bs-4-lr-0.001-wd-0/attn_pdhg_adamw-lr-0.001-constant-linear-fbe0ebc2a3df4200c0b42d3d91e267fc.json\") as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "logger = SimpleNamespace(**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ddaae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_residuals(residuals_by_layer):\n",
    "    # residuals_by_layer: {layer_idx: {'r1': [...], 'r2': [...], 'r1_rel': [...], 'r2_rel': [...]} }\n",
    "    layer_indices = sorted(residuals_by_layer.keys())\n",
    "    n_layers = len(layer_indices)\n",
    "\n",
    "    fig, ax = plt.subplots(n_layers, 2, figsize=(10, 3 * n_layers), sharex=True)\n",
    "    if n_layers == 1:\n",
    "        ax = ax.reshape(1, 2)\n",
    "\n",
    "    spec = [\n",
    "        ('r1', 'r2', 'Absolute residuals', r'$r_1$', r'$r_2$'),\n",
    "        ('r1_rel', 'r2_rel', 'Relative residuals', r'$r_1^{\\text{rel}}$', r'$r_2^{\\text{rel}}$')\n",
    "    ]\n",
    "\n",
    "    for row, layer_idx in enumerate(layer_indices):\n",
    "        layer_res = residuals_by_layer[layer_idx]\n",
    "        for col, (k1, k2, title, l1, l2) in enumerate(spec):\n",
    "            d1, d2 = layer_res.get(k1, []), layer_res.get(k2, [])\n",
    "            if len(d1) and len(d2):\n",
    "                ax[row, col].plot(d1, label=l1)\n",
    "                ax[row, col].plot(d2, label=l2)\n",
    "                ax[row, col].set(yscale='log', title=f\"Layer {layer_idx} – {title}\", xlabel='iteration')\n",
    "                ax[row, col].grid(True, which='both', ls='--', alpha=0.4)\n",
    "                ax[row, col].legend()\n",
    "            else:\n",
    "                ax[row, col].axis('off')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "print(f\"iteration={it}\")\n",
    "plot_residuals(logger.pdhg_residuals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5258fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 10\n",
    "print(f\"iteration={it}\")\n",
    "plot_residuals(logger.pdhg_residuals[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f3e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 100\n",
    "print(f\"iteration={it}\")\n",
    "plot_residuals(logger.pdhg_residuals[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(logger.pdhg_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9564d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layer_indices = sorted({\n",
    "    layer_idx\n",
    "    for it_res in logger.pdhg_residuals\n",
    "    for layer_idx in it_res.keys()\n",
    "})\n",
    "\n",
    "calls = np.arange(len(logger.pdhg_residuals))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, layer_idx in zip(axes, layer_indices):\n",
    "    W_q_norms, W_k_norms, G_q_norms, G_k_norms = [], [], [], []\n",
    "    for it_res in logger.pdhg_residuals:\n",
    "        layer_res = it_res.get(layer_idx)\n",
    "        if layer_res is None:\n",
    "            W_q_norms.append(np.nan); W_k_norms.append(np.nan)\n",
    "            G_q_norms.append(np.nan); G_k_norms.append(np.nan)\n",
    "        else:\n",
    "            W_q_norms.append(layer_res[\"W_q_norm\"])\n",
    "            W_k_norms.append(layer_res[\"W_k_norm\"])\n",
    "            G_q_norms.append(layer_res[\"G_q_norm\"])\n",
    "            G_k_norms.append(layer_res[\"G_k_norm\"])\n",
    "\n",
    "    ax.plot(calls, W_q_norms, label=\"W_q\")\n",
    "    ax.plot(calls, W_k_norms, label=\"W_k\")\n",
    "    ax.plot(calls, G_q_norms, label=\"G_q\")\n",
    "    ax.plot(calls, G_k_norms, label=\"G_k\")\n",
    "    ax.set_title(f\"Layer {layer_idx}\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.grid(True, which=\"both\", ls=\"--\", alpha=0.4)\n",
    "\n",
    "axes[0].set_ylabel(\"Norm value\")\n",
    "axes[2].set_xlabel(\"PDHG call index\")\n",
    "axes[3].set_xlabel(\"PDHG call index\")\n",
    "\n",
    "# One shared legend for all axes\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper right\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498d346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
