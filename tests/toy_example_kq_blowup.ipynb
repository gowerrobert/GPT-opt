{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa58fa2e",
   "metadata": {},
   "source": [
    "# Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9097ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange, einsum\n",
    "import einx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b93949",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 4\n",
    "sequence =  100\n",
    "d_in = 64\n",
    "d_out = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f2c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = torch.randn(batch, sequence, d_in)\n",
    "A = torch.randn(d_out, d_in)\n",
    "\n",
    "## Basic implementation\n",
    "Y1 = D @ A.T\n",
    "# Hard to tell the input and output shapes and what they mean.\n",
    "# What shapes can D and A have, and do any of these have unexpected behavior?\n",
    "\n",
    "## Einsum is self-documenting and robust\n",
    "# D A -> Y\n",
    "Y2 = einsum(D, A, \"batch sequence d_in, d_out d_in -> batch sequence d_out\")\n",
    "## Or, a batched version where D can have any leading dimensions but A is constrained.\n",
    "Y3 = einsum(D, A, \"... d_in, d_out d_in -> ... d_out\")\n",
    "\n",
    "assert torch.allclose(Y1, Y2) and torch.allclose(Y1, Y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e877e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn(64, 128, 128, 3) # (batch, height, width, channel)\n",
    "dim_by = torch.linspace(start=0.0, end=1.0, steps=10)\n",
    "## Reshape and multiply\n",
    "dim_value = rearrange(dim_by, \"dim_value -> 1 dim_value 1 1 1\")\n",
    "images_rearr = rearrange(images, \"b height width channel -> b 1 height width channel\")\n",
    "dimmed_images1 = images_rearr * dim_value\n",
    "## Or in one go:\n",
    "dimmed_images2 = einsum(\n",
    "images, dim_by,\n",
    "\"batch height width channel, dim_value -> batch dim_value height width channel\"\n",
    ")\n",
    "assert torch.allclose(dimmed_images1, dimmed_images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have a batch of images represented as a tensor of shape (batch, height, width,\n",
    "# channel), and we want to perform a linear transformation across all pixels of the image, but this\n",
    "# transformation should happen independently for each channel. Our linear transformation is\n",
    "# represented as a matrix B of shape (height × width, height × width).\n",
    "channels_last = torch.randn(64, 32, 32, 3) # (batch, height, width, channel)\n",
    "B = torch.randn(32*32, 32*32)\n",
    "## Rearrange an image tensor for mixing across all pixels\n",
    "channels_last_flat = channels_last.view(\n",
    "-1, channels_last.size(1) * channels_last.size(2), channels_last.size(3)\n",
    ")\n",
    "channels_first_flat = channels_last_flat.transpose(1, 2)\n",
    "channels_first_flat_transformed = channels_first_flat @ B.T\n",
    "channels_last_flat_transformed = channels_first_flat_transformed.transpose(1, 2)\n",
    "channels_last_transformed = channels_last_flat_transformed.view(*channels_last.shape)\n",
    "# Instead, using einops:\n",
    "height = width = 32\n",
    "## Rearrange replaces clunky torch view + transpose\n",
    "channels_first = rearrange(\n",
    "channels_last,\n",
    "\"batch height width channel -> batch channel (height width)\"\n",
    ")\n",
    "channels_first_transformed = einsum(\n",
    "channels_first, B,\n",
    "\"batch channel pixel_in, pixel_out pixel_in -> batch channel pixel_out\"\n",
    ")\n",
    "channels_last_transformed = rearrange(\n",
    "channels_first_transformed,\n",
    "\"batch channel (height width) -> batch height width channel\",\n",
    "height=height, width=width\n",
    ")\n",
    "# Or, if you’re feeling crazy: all in one go using einx.dot (einx equivalent of einops.einsum)\n",
    "height = width = 32\n",
    "channels_last_transformed = einx.dot(\n",
    "\"batch row_in col_in channel, (row_out col_out) (row_in col_in)\"\n",
    "\"-> batch row_out col_out channel\",\n",
    "channels_last, B,\n",
    "col_in=width, col_out=width\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, device=None, dtype=None):\n",
    "        \"\"\" Construct a linear transformation module. This function should accept the following parameters:\n",
    "        in_features: int final dimension of the input\n",
    "        out_features: int final dimension of the output\n",
    "        device: torch.device | None = None Device to store the parameters on\n",
    "        dtype: torch.dtype | None = None Data type of the parameters\n",
    "        \"\"\"\n",
    "        W = torch.empty(out_features, in_features, device=device, dtype=dtype)\n",
    "        self.weight = nn.Parameter(nn.init.trunc_normal_(W, a=-3, b=3))\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        # Apply the linear transformation to the input.\n",
    "        res = einsum(self.weight, x, \"d_out d_in, ... d_in -> ... d_out\")\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None):\n",
    "        \"\"\" Construct an embedding module. This function should accept the following parameters:\n",
    "        num_embeddings: int Size of the vocabulary \n",
    "        embedding_dim: int Dimension of the embedding vectors, i.e., dmodel\n",
    "        device: torch.device | None = None Device to store the parameters on\n",
    "        dtype: torch.dtype | None = None Data type of the parameters\n",
    "        \"\"\" \n",
    "        E = torch.empty(num_embeddings, embedding_dim, device=device, dtype=dtype)\n",
    "        self.embeddings = nn.Parameter(nn.init.trunc_normal_(E, a=-3, b=3))\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        #  Lookup the embedding vectors\n",
    "        return self.embeddings[token_ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9037cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None):\n",
    "        \"\"\" Construct the RMSNorm module. This function should accept the following parameters:\n",
    "        d_model: int Hidden dimension of the model\n",
    "        eps: float = 1e-5 Epsilon value for numerical stability\n",
    "        device: torch.device | None = None Device to store the parameters on\n",
    "        dtype: torch.dtype | None = None Data type of the parameters\n",
    "        \"\"\"\n",
    "        self.d_model = d_model \n",
    "        self.eps = eps\n",
    "        self.gain = nn.Parameter(torch.ones(d_model, device=device, dtype=dtype))\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Process an input tensor of shape\n",
    "        # (batch_size, sequence_length, d_model) and return a tensor of the same shape.\n",
    "        in_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        x2_over_g = einsum(x.pow(2), 1/self.gain, \"batch_size sequence_length d_model, d_model -> batch_size sequence_length d_model\")\n",
    "        rms = (self.eps + x2_over_g).sqrt()\n",
    "        res = (x * self.gain) / rms \n",
    "        return res.to(in_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ccb0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
