{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e98ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gptopt.optim.attn_utils import * \n",
    "from gptopt.optim.linop import * \n",
    "from gptopt.gpt_model import *\n",
    "from einops import rearrange, einsum\n",
    "from utils import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a1cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from gptopt.utils import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85252805",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxit = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf8189",
   "metadata": {},
   "source": [
    "# Linea operator and its adjoint\n",
    "$$\\mathcal{A}(Z) = \n",
    "\\begin{bmatrix}\n",
    "    (Z^1_1)^\\top A^1_1 + (A^1_2)^\\top Z^1_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    (Z^h_1)^\\top A^h_1 + (A^h_2)^\\top Z^h_2\n",
    "\\end{bmatrix},\n",
    "\\qquad \n",
    "\\mathcal{A}^*(Y) = (A^1_1Y_1^\\top, A^1_2Y_1, \\ldots, A^h_1Y_h^\\top, A^h_2Y_h)\n",
    "$$\n",
    "\n",
    "### Vectorization\n",
    "$$\n",
    "K = [(A_1^\\top \\otimes I_n)P, I_n \\otimes A_2^\\top],\n",
    "\\qquad \n",
    "K^\\top = \\begin{bmatrix}\n",
    "P^\\top(A_1 \\otimes I_n) \\\\\n",
    "I_n \\otimes A_2\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "where $P$ is a permutation matrix s.t $P\\text{vec}(Z^\\top) = \\text{vec}(Z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b443d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "n_head = 5\n",
    "dtype = torch.float64\n",
    "for (m, n) in [(30, 60), (60, 30), (60, 60)]:\n",
    "    # print(f\"{m}x{n}\")\n",
    "    for _ in range(5):  \n",
    "        A1 = torch.randn((n_head * m, n), device=device).to(dtype)\n",
    "        A2 = torch.randn((n_head * m, n), device=device).to(dtype)\n",
    "        Z = torch.randn((2 * n_head * m, n), device=A2.device, dtype=A2.dtype)\n",
    "        Y = torch.randn((n_head * n, n), device=A2.device, dtype=A2.dtype)\n",
    "\n",
    "        A_linop = attn_linop_from_matrices_heads(A1, A2, n_head=n_head)\n",
    "\n",
    "        Az = A_linop.matvec(Z)\n",
    "        Aty = A_linop.rmatvec(Y) \n",
    "        tr1 = (Az * Y).sum()\n",
    "        tr2 = (Z * Aty).sum()\n",
    "        assert torch.allclose(tr1, tr2)\n",
    "\n",
    "        A1_heads, A2_heads = A1_A2_unpack_heads(A_linop.A1, A_linop.A2, n_head)\n",
    "        Z1_heads, Z2_heads = Z_unpack_Z1_Z2_heads(Z, n_head=n_head)\n",
    "        Y_heads = rearrange(Y, \"(n_head n_embd1) n_embd2 -> n_head n_embd1 n_embd2\",\n",
    "                             n_head=n_head)\n",
    "\n",
    "        vec_Kz = torch.empty_like(Az.flatten())\n",
    "        vec_Kty = torch.empty_like(Aty.flatten())\n",
    "\n",
    "        Az_heads = rearrange(Az, \"(n_head n_emb1) n_emb2 -> n_head n_emb1 n_emb2\", n_head=n_head)\n",
    "        Aty_heads = rearrange(Aty, \"(n_head zs n_att) n_embd -> n_head zs n_att n_embd\", n_head=n_head, zs=2)\n",
    "        vec_Az = torch.empty_like(Az.flatten())\n",
    "        vec_Aty = torch.empty_like(Aty.flatten())\n",
    "        for h in range(n_head):\n",
    "            K = matcal_A_to_kron_Kron(A1_heads[h], A2_heads[h]) \n",
    "            Kz = K @ torch.cat([Z1_heads[h].reshape(-1), Z2_heads[h].T.reshape(-1)], dim=0)\n",
    "            vec_Kz[h*n**2 : (h+1)*n**2] = Kz\n",
    "            vec_Az[h*n**2 : (h+1)*n**2] = Az_heads[h].T.reshape(-1)\n",
    " \n",
    "            KTy = K.T @ Y_heads[h].T.reshape(-1)\n",
    "            vec_Kty[2*h*m*n : 2*(h+1)*m*n] = KTy\n",
    "            vec_Aty[2*h*m*n : 2*h*m*n + m*n] = Aty_heads[h, 0].reshape(-1)\n",
    "            vec_Aty[2*h*m*n + m*n : 2*(h+1)*m*n] = Aty_heads[h, 1].T.reshape(-1)\n",
    "        \n",
    "        assert torch.allclose(vec_Kz, vec_Az, atol=1e-5)\n",
    "        assert torch.allclose(vec_Kty, vec_Aty, atol=1e-5)\n",
    "\n",
    "\n",
    "print(\"PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480f9cc",
   "metadata": {},
   "source": [
    "# Kronecker matrix, and packing / unpacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd0dd759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "n_head = 3\n",
    "dtype = torch.float64\n",
    "for (m, n) in [(15, 30), (30, 15), (20, 20)]:\n",
    "    # print(f\"{m}x{n}\")\n",
    "    for _ in range(5):  \n",
    "        A1 = torch.randn((n_head * m, n), device=device).to(dtype)\n",
    "        A2 = torch.randn((n_head * m, n), device=device).to(dtype)\n",
    "        Z = torch.randn((2 * n_head * m, n), device=A2.device, dtype=A2.dtype)\n",
    "        Y = torch.randn((n_head * n, n), device=A2.device, dtype=A2.dtype)\n",
    "\n",
    "        A_linop = attn_linop_from_matrices_heads(A1, A2, n_head=n_head) \n",
    "        \n",
    "        K = kron_mat_A_linop_heads(A1, A2, n_head)\n",
    "\n",
    "        z = pack_Z(Z, m, n, n_head)\n",
    "        Az = pack_Y(A_linop.mv(Z), n, n_head)\n",
    "        assert torch.allclose(K @ z, Az, rtol=1e-5)\n",
    "\n",
    "        y = pack_Y(Y, n, n_head)\n",
    "        Aty = pack_Z(A_linop.rmv(Y), m, n, n_head)\n",
    "        assert torch.allclose(K.T @ y, Aty, rtol=1e-5)\n",
    " \n",
    "\n",
    "print(\"PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e1d63b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "n_head = 7\n",
    "for (m, n) in [(35, 60), (20, 45), (60, 20), (50, 50)]:\n",
    "    for _ in range(5):\n",
    "        Z = torch.randn(2*m*n_head, n) \n",
    "        Z1, Z2 = rearrange(Z_unpack_Z1_Z2_heads(Z, n_head=n_head), \n",
    "                           \"zs n_head n_att n_embd -> zs (n_head n_att) n_embd\")\n",
    "        assert torch.allclose(Z, Z1_Z2_pack_Z_heads(Z1, Z2, n_head=n_head))\n",
    "\n",
    "print(\"PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5234b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8af12a3c",
   "metadata": {},
   "source": [
    "## Diagonal scaling s.t. $\\|R^{1/2} A \\Gamma^{1/2}\\|_{op} < 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a00323",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d523c9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00] n=9 m=5  sigma_max=0.774346 sigma_powit=0.774346  -> OK\n",
      "[01] n=4 m=6  sigma_max=0.866952 sigma_powit=0.866952  -> OK\n",
      "[02] n=4 m=5  sigma_max=0.868909 sigma_powit=0.868909  -> OK\n",
      "[03] n=8 m=2  sigma_max=0.935572 sigma_powit=0.935506  -> OK\n",
      "[04] n=9 m=7  sigma_max=0.762751 sigma_powit=0.762751  -> OK\n",
      "[05] n=9 m=7  sigma_max=0.756849 sigma_powit=0.756849  -> OK\n",
      "[06] n=4 m=7  sigma_max=0.825347 sigma_powit=0.825347  -> OK\n",
      "[07] n=7 m=2  sigma_max=0.917056 sigma_powit=0.917056  -> OK\n",
      "[08] n=3 m=6  sigma_max=0.876481 sigma_powit=0.876481  -> OK\n",
      "[09] n=3 m=9  sigma_max=0.835567 sigma_powit=0.835567  -> OK\n",
      "[10] n=8 m=8  sigma_max=0.729278 sigma_powit=0.729278  -> OK\n",
      "[11] n=3 m=8  sigma_max=0.845582 sigma_powit=0.845582  -> OK\n",
      "[12] n=5 m=3  sigma_max=0.891582 sigma_powit=0.891582  -> OK\n",
      "[13] n=6 m=8  sigma_max=0.757297 sigma_powit=0.757297  -> OK\n",
      "[14] n=4 m=9  sigma_max=0.805240 sigma_powit=0.805240  -> OK\n",
      "[15] n=3 m=5  sigma_max=0.911359 sigma_powit=0.910819  -> OK\n",
      "[16] n=3 m=6  sigma_max=0.874926 sigma_powit=0.874926  -> OK\n",
      "[17] n=4 m=7  sigma_max=0.787412 sigma_powit=0.787411  -> OK\n",
      "[18] n=7 m=4  sigma_max=0.820282 sigma_powit=0.820283  -> OK\n",
      "[19] n=8 m=2  sigma_max=0.956616 sigma_powit=0.956616  -> OK\n",
      "\n",
      "Summary: 20 / 20 passed (eta=0.9900000098999999).\n"
     ]
    }
   ],
   "source": [
    "num_cases=20; seed=1234; verbose=True\n",
    "torch.manual_seed(seed)\n",
    "fails = 0\n",
    "for t in range(num_cases):\n",
    "    n  = torch.randint(3, 10, ()).item()    # keep small (explicit K is O(n^3))\n",
    "    m = torch.randint(2, 10, ()).item() \n",
    "    A2 = torch.randn(n_head * m, n)\n",
    "    A1 = torch.randn(n_head * m, n)\n",
    "    A_linop = attn_linop_from_matrices_heads(A1, A2, n_head=n_head)\n",
    "\n",
    "    # scaling\n",
    "    eta=0.99; eps=1e-8\n",
    "    Rm, Gamma = diagonal_scaling_heads_op_norm(A_linop, eta=eta, eps=eps)\n",
    "    G1, G2 = rearrange(Gamma, \"(n_head zs n_att) n_embed -> zs n_head n_att n_embed\", n_head=n_head, zs=2)\n",
    "\n",
    "    # explicit K\n",
    "    K = kron_mat_A_linop_heads(A_linop.A1, A_linop.A2, n_head)\n",
    "    R_diag_sqrt = torch.diag(pack_Y(torch.sqrt(Rm), n, n_head))\n",
    "    G_diag_sqrt = torch.diag(pack_Z(torch.sqrt(Gamma), m, n, n_head))\n",
    "    # R and G matrices from K\n",
    "    rows = K.norm(dim=1, p=1)   # (h*n*n,)\n",
    "    cols = K.norm(dim=0, p=1)   # (2*h*m*n,)\n",
    "    R2 = torch.where(rows > 0, eta**0.5 / torch.sqrt(rows + eps), torch.zeros_like(rows))\n",
    "    G2 = torch.where(cols > 0, eta**0.5 / torch.sqrt(cols + eps), torch.zeros_like(cols))\n",
    "    assert torch.allclose(R2, R_diag_sqrt.diagonal(), rtol=1e-3) \\\n",
    "        and torch.allclose(G2, G_diag_sqrt.diagonal(), rtol=1e-3)\n",
    "    # bounds and spectral norm \n",
    "    sigma_max = torch.linalg.svdvals(R_diag_sqrt @ K @ G_diag_sqrt)[0].item() \n",
    "    sigma_powit = op_norm_power_iteration(A_linop, Rm.sqrt(), Gamma.sqrt(), num_iters=500)\n",
    "    ok = (sigma_max <= eta*(1+1e-8))\n",
    "    if verbose:\n",
    "        print(f\"[{t:02d}] n={n} m={m}  \"\n",
    "                f\"{sigma_max=:.6f} {sigma_powit=:.6f}  -> {'OK' if ok else 'FAIL'}\")\n",
    "    fails += 0 if ok else 1\n",
    "print(f\"\\nSummary: {num_cases - fails} / {num_cases} passed (eta={eta*(1+1e-8)}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499c577",
   "metadata": {},
   "source": [
    "# Test slicing in `CausalSelfAttention` for $\\mathcal{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7060ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = n_embed = 768   \n",
    "n_head = 12 \n",
    "B = batch_size = 32\n",
    "T = context_length = 1024\n",
    "\n",
    "attn = CausalSelfAttention(GPTConfig(n_embd=n_embed, n_head=n_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52da8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = attn.c_attn.weight\n",
    "# A1 = W_q, A2 = W_k\n",
    "A1, A2   = p[:n_embed, :], p[n_embed:2 * n_embed, :]\n",
    "x = torch.randn((batch_size, context_length, n_embed), device=p.device, dtype=p.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee1db63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv = attn.c_attn(x)\n",
    "q, k, v = rearrange(qkv, \"batch seqlen (size n) -> size batch seqlen n\", size=3)\n",
    "\n",
    "q2, k2, v2 = qkv.split(n_embed, dim=2)\n",
    "assert torch.allclose(q, q2) and torch.allclose(k, k2) and torch.allclose(v, v2)\n",
    "\n",
    "# split over n_head\n",
    "q_heads = rearrange(q, \"batch seqlen (n_head n_att) -> batch n_head seqlen n_att\", n_head=n_head)\n",
    "k_heads = rearrange(k, \"batch seqlen (n_head n_att) -> batch n_head seqlen n_att\", n_head=n_head)\n",
    "v_heads = rearrange(v, \"batch seqlen (n_head n_att) -> batch n_head seqlen n_att\", n_head=n_head)\n",
    "\n",
    "k2 = k.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "q2 = q.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "v2 = v.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs) \n",
    "assert torch.allclose(q_heads, q2) and torch.allclose(k_heads, k2) and torch.allclose(v_heads, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a87e7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qk2 = (q2 @ k2.transpose(-2, -1)) \n",
    "qk = einsum(q_heads, k_heads, \"batch n_head seqlen1 n_att, batch n_head seqlen2 n_att\\\n",
    "            -> batch n_head seqlen1 seqlen2\")\n",
    "assert torch.allclose(qk, qk2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3433010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (n_head, n_att, n_embd) \n",
    "A1_heads, A2_heads = A1_A2_unpack_heads(A1, A2, n_head)\n",
    "Wq_heads, Wk_heads = A1_heads, A2_heads \n",
    "XWq = einsum(x, Wq_heads, \"batch seqlen n_embd, n_head n_att n_embd \\\n",
    "                           -> batch n_head seqlen n_att\")\n",
    "XWk = einsum(x, Wk_heads, \"batch seqlen n_embd, n_head n_att n_embd \\\n",
    "                           -> batch n_head seqlen n_att\")\n",
    "QK = einsum(XWq, XWk, \"batch n_head seqlen1 n_att, batch n_head seqlen2 n_att\\\n",
    "            -> batch n_head seqlen1 seqlen2\")\n",
    "assert torch.allclose(QK, qk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec513258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    attn = CausalSelfAttention(GPTConfig(n_embd=n_embed, n_head=n_head))\n",
    "    x = torch.randn((batch_size, context_length, n_embed), device=p.device, dtype=p.dtype)\n",
    "\n",
    "    res2 = attn(x)\n",
    "\n",
    "    p = attn.c_attn.weight\n",
    "    # A1 = W_q, A2 = W_k\n",
    "    A1, A2, Wv   = p[:n_embed, :], p[n_embed:2 * n_embed, :], p[2 * n_embed:3 * n_embed, :]\n",
    "    # (n_head, n_att, n_embd) \n",
    "    A1_heads, A2_heads = A1_A2_unpack_heads(A1, A2, n_head)\n",
    "    Wq_heads, Wk_heads = A1_heads, A2_heads\n",
    "    Wv_heads = rearrange(Wv, \"(n_head n_att) n_embd -> n_head n_att n_embd\",\n",
    "                    n_head=n_head) \n",
    "    q = einsum(x, Wq_heads, \"batch seqlen n_embd, n_head n_att n_embd \\\n",
    "                            -> batch n_head seqlen n_att\")\n",
    "    k = einsum(x, Wk_heads, \"batch seqlen n_embd, n_head n_att n_embd \\\n",
    "                            -> batch n_head seqlen n_att\")\n",
    "    v = einsum(x, Wv_heads, \"batch seqlen n_embd, n_head n_att n_embd \\\n",
    "                            -> batch n_head seqlen n_att\")\n",
    "    cos, sin = attn.rope.get_embed(T, x.device, x.dtype)\n",
    "    q = apply_rotary_pos_emb(q, cos, sin)\n",
    "    k = apply_rotary_pos_emb(k, cos, sin)\n",
    "\n",
    "    res = einsum(q, k, \"batch n_head seqlen1 n_att, batch n_head seqlen2 n_att\\\n",
    "                -> batch n_head seqlen1 seqlen2\") / (k.shape[-1])**0.5\n",
    "    res = res.masked_fill(attn.causal_mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "    res = F.softmax(res, dim=-1)       \n",
    "    res = einsum(res, v, \"batch n_head seqlen1 seqlen2, batch n_head seqlen2 n_embd \\\n",
    "                        -> batch n_head seqlen1 n_embd\")     \n",
    "    res = rearrange(res, \"batch n_head seqlen n_embd -> batch seqlen (n_head n_embd)\")\n",
    "    res = attn.c_proj(res)\n",
    "\n",
    "    assert torch.allclose(res, res2)\n",
    "\n",
    "print(\"PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d48929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
