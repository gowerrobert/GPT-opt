{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e98ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/tparshakova/Documents/GPT-opt/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gptopt.optim.attn_utils import * \n",
    "from gptopt.optim.linop import * \n",
    "from gptopt.gpt_model import *\n",
    "from einops import rearrange, einsum\n",
    "from utils import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a1cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from gptopt.utils import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85252805",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxit = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf8189",
   "metadata": {},
   "source": [
    "# Linea operator and its adjoint\n",
    "$$\\mathcal{A}(Z) = \n",
    "\\begin{bmatrix}\n",
    "    (Z^1_1)^\\top A^1_1 + (A^1_2)^\\top Z^1_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    (Z^h_1)^\\top A^h_1 + (A^h_2)^\\top Z^h_2\n",
    "\\end{bmatrix},\n",
    "\\qquad \n",
    "\\mathcal{A}^*(Y) = (A^1_1Y_1^\\top, A^1_2Y_1, \\ldots, A^h_1Y_h^\\top, A^h_2Y_h)\n",
    "$$\n",
    "\n",
    "### Vectorization\n",
    "$$\n",
    "K = [(A_1^\\top \\otimes I_n)P, I_n \\otimes A_2^\\top],\n",
    "\\qquad \n",
    "K^\\top = \\begin{bmatrix}\n",
    "P^\\top(A_1 \\otimes I_n) \\\\\n",
    "I_n \\otimes A_2\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "where $P$ is a permutation matrix s.t $P\\text{vec}(Z^\\top) = \\text{vec}(Z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b443d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "from gptopt.optim.least_squares import matcal_A_to_kron_Kron\n",
    "\n",
    "\n",
    "n_head = 5\n",
    "dtype = torch.float64\n",
    "for (m, n) in [(30, 60), (60, 30), (60, 60)]:\n",
    "    # print(f\"{m}x{n}\")\n",
    "    for _ in range(5):  \n",
    "        A1 = torch.randn((n_head * m, n), device=device).to(dtype)\n",
    "        A2 = torch.randn((n_head * m, n), device=device).to(dtype)\n",
    "        Z = torch.randn((2 * n_head * m, n), device=A2.device, dtype=A2.dtype)\n",
    "        Y = torch.randn((n_head * n, n), device=A2.device, dtype=A2.dtype)\n",
    "\n",
    "        A_linop = attn_linop_from_matrices_heads(A1, A2, n_head=n_head)\n",
    "\n",
    "        Az = A_linop.matvec(Z)\n",
    "        Aty = A_linop.rmatvec(Y) \n",
    "        tr1 = (Az * Y).sum()\n",
    "        tr2 = (Z * Aty).sum()\n",
    "        assert torch.allclose(tr1, tr2)\n",
    "\n",
    "        A1_heads, A2_heads = A1_A2_unpack_heads(A_linop.A1, A_linop.A2, n_head)\n",
    "        Z1_heads, Z2_heads = Z_unpack_Z1_Z2_heads(Z, n_head=n_head)\n",
    "        Y_heads = rearrange(Y, \"(n_head n_embd1) n_embd2 -> n_head n_embd1 n_embd2\",\n",
    "                             n_head=n_head)\n",
    "\n",
    "        vec_Kz = torch.empty_like(Az.flatten())\n",
    "        vec_Kty = torch.empty_like(Aty.flatten())\n",
    "\n",
    "        Az_heads = rearrange(Az, \"(n_head n_emb1) n_emb2 -> n_head n_emb1 n_emb2\", n_head=n_head)\n",
    "        Aty_heads = rearrange(Aty, \"(n_head zs n_att) n_embd -> n_head zs n_att n_embd\", n_head=n_head, zs=2)\n",
    "        vec_Az = torch.empty_like(Az.flatten())\n",
    "        vec_Aty = torch.empty_like(Aty.flatten())\n",
    "        for h in range(n_head):\n",
    "            K = matcal_A_to_kron_Kron(A1_heads[h], A2_heads[h]) \n",
    "            Kz = K @ torch.cat([Z1_heads[h].reshape(-1), Z2_heads[h].T.reshape(-1)], dim=0)\n",
    "            vec_Kz[h*n**2 : (h+1)*n**2] = Kz\n",
    "            vec_Az[h*n**2 : (h+1)*n**2] = Az_heads[h].T.reshape(-1)\n",
    " \n",
    "            KTy = K.T @ Y_heads[h].T.reshape(-1)\n",
    "            vec_Kty[2*h*m*n : 2*(h+1)*m*n] = KTy\n",
    "            vec_Aty[2*h*m*n : 2*h*m*n + m*n] = Aty_heads[h, 0].reshape(-1)\n",
    "            vec_Aty[2*h*m*n + m*n : 2*(h+1)*m*n] = Aty_heads[h, 1].T.reshape(-1)\n",
    "        \n",
    "        assert torch.allclose(vec_Kz, vec_Az, atol=1e-5)\n",
    "        assert torch.allclose(vec_Kty, vec_Aty, atol=1e-5)\n",
    "\n",
    "\n",
    "print(\"PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499c577",
   "metadata": {},
   "source": [
    "# Test slicing in `CausalSelfAttention` for $\\mathcal{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7060ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = n_embed = 768   \n",
    "n_head = 12 \n",
    "B = batch_size = 32\n",
    "T = context_length = 1024\n",
    "\n",
    "attn = CausalSelfAttention(GPTConfig(n_embd=n_embed, n_head=n_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52da8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = attn.c_attn.weight\n",
    "# A1 = W_q, A2 = W_k\n",
    "A1, A2   = p[:n_embed, :], p[n_embed:2 * n_embed, :]\n",
    "x = torch.randn((batch_size, context_length, n_embed), device=p.device, dtype=p.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee1db63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv = attn.c_attn(x)\n",
    "q, k, v = rearrange(qkv, \"batch seqlen (size n) -> size batch seqlen n\", size=3)\n",
    "\n",
    "q2, k2, v2 = qkv.split(n_embed, dim=2)\n",
    "assert torch.allclose(q, q2) and torch.allclose(k, k2) and torch.allclose(v, v2)\n",
    "\n",
    "# split over n_head\n",
    "q_heads = rearrange(q, \"batch seqlen (n_head n_att) -> batch n_head seqlen n_att\", n_head=n_head)\n",
    "k_heads = rearrange(k, \"batch seqlen (n_head n_att) -> batch n_head seqlen n_att\", n_head=n_head)\n",
    "v_heads = rearrange(v, \"batch seqlen (n_head n_att) -> batch n_head seqlen n_att\", n_head=n_head)\n",
    "\n",
    "k2 = k.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "q2 = q.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "v2 = v.view(B, T, n_head, C // n_head).transpose(1, 2) # (B, nh, T, hs) \n",
    "assert torch.allclose(q_heads, q2) and torch.allclose(k_heads, k2) and torch.allclose(v_heads, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a87e7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qk2 = (q2 @ k2.transpose(-2, -1)) \n",
    "qk = einsum(q_heads, k_heads, \"batch n_head seqlen1 n_att, batch n_head seqlen2 n_att\\\n",
    "            -> batch n_head seqlen1 seqlen2\")\n",
    "assert torch.allclose(qk, qk2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3433010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (n_head, n_att, n_embd) \n",
    "A1_heads, A2_heads = A1_A2_unpack_heads(A1, A2, n_head)\n",
    "Wq_heads, Wk_heads = A1_heads, A2_heads \n",
    "XWq = einsum(x, Wq_heads, \"batch seqlen n_embd, n_head n_att n_embd \\\n",
    "                           -> batch n_head seqlen n_att\")\n",
    "XWk = einsum(x, Wk_heads, \"batch seqlen n_embd, n_head n_att n_embd \\\n",
    "                           -> batch n_head seqlen n_att\")\n",
    "QK = einsum(XWq, XWk, \"batch n_head seqlen1 n_att, batch n_head seqlen2 n_att\\\n",
    "            -> batch n_head seqlen1 seqlen2\")\n",
    "assert torch.allclose(QK, qk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec513258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    attn = CausalSelfAttention(GPTConfig(n_embd=n_embed, n_head=n_head))\n",
    "    x = torch.randn((batch_size, context_length, n_embed), device=p.device, dtype=p.dtype)\n",
    "\n",
    "    res2 = attn(x)\n",
    "\n",
    "    p = attn.c_attn.weight\n",
    "    # A1 = W_q, A2 = W_k\n",
    "    A1, A2, Wv   = p[:n_embed, :], p[n_embed:2 * n_embed, :], p[2 * n_embed:3 * n_embed, :]\n",
    "    # (n_head, n_att, n_embd) \n",
    "    A1_heads, A2_heads = A1_A2_unpack_heads(A1, A2, n_head)\n",
    "    Wq_heads, Wk_heads = A1_heads, A2_heads\n",
    "    Wv_heads = rearrange(Wv, \"(n_head n_att) n_embd -> n_head n_att n_embd\",\n",
    "                    n_head=n_head) \n",
    "    q = einsum(x, Wq_heads, \"batch seqlen n_embd, n_head n_att n_embd \\\n",
    "                            -> batch n_head seqlen n_att\")\n",
    "    k = einsum(x, Wk_heads, \"batch seqlen n_embd, n_head n_att n_embd \\\n",
    "                            -> batch n_head seqlen n_att\")\n",
    "    v = einsum(x, Wv_heads, \"batch seqlen n_embd, n_head n_att n_embd \\\n",
    "                            -> batch n_head seqlen n_att\")\n",
    "    cos, sin = attn.rope.get_embed(T, x.device, x.dtype)\n",
    "    q = apply_rotary_pos_emb(q, cos, sin)\n",
    "    k = apply_rotary_pos_emb(k, cos, sin)\n",
    "\n",
    "    res = einsum(q, k, \"batch n_head seqlen1 n_att, batch n_head seqlen2 n_att\\\n",
    "                -> batch n_head seqlen1 seqlen2\") / (k.shape[-1])**0.5\n",
    "    res = res.masked_fill(attn.causal_mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "    res = F.softmax(res, dim=-1)       \n",
    "    res = einsum(res, v, \"batch n_head seqlen1 seqlen2, batch n_head seqlen2 n_embd \\\n",
    "                        -> batch n_head seqlen1 n_embd\")     \n",
    "    res = rearrange(res, \"batch n_head seqlen n_embd -> batch seqlen (n_head n_embd)\")\n",
    "    res = attn.c_proj(res)\n",
    "\n",
    "    assert torch.allclose(res, res2)\n",
    "\n",
    "print(\"PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f53ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
