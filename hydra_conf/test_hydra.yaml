defaults:
  - gpt_model: gpt-small
  - _self_

optimizer_params:
  name: adamw
  lr: 0.001
  betas: [0.95, 0.95]
  weight_decay: 0.1
  lr_schedule: constant-linear
  warm_up_fraction: 0.4

logging_params:
  val_tokens_processed: 2048 #2^23
  log_step: 50
  val_step: 50
  save_ckpt_step: 500
  load_ckpt_step: 0
  keep_last: 2
  ckpt_dir: ""  # "dap_outputs/checkpoints"
  # results_dir: "dap_outputs/hydra-results/my_experiment"
  # wandb:
  #   project: "polar-express"
