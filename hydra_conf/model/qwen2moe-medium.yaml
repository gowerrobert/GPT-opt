config:
  vocab_size: 50257
  hidden_size: 768
  num_hidden_layers: 16
  # 64-d heads
  num_attention_heads: 12
  num_key_value_heads: 12
  # MoE
  num_experts: 8                # increase to 16/32 to raise total capacity at same active compute
  num_experts_per_tok: 2
  moe_intermediate_size: 4096   # per-expert FFN width (~5.3x H)
  shared_expert_intermediate_size: 4096  # shared FFN width
  decoder_sparse_step: 1        # MoE in every layer

  tie_word_embeddings: true
  qkv_bias: true
  trust_remote_code: false
  source: "huggingface"
name: Qwen2Moe

# Expected (approx):
# - Active params/token ≈ 491M  => compute-optimal ~10B tokens
# - Total params ≈ 1.44B (tied embeddings)
# Fit tips: bf16, gradient checkpointing, use_cache=False, micro-batch 16–64 at T=1024.