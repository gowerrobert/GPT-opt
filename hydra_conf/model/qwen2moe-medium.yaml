config:
  vocab_size: 50257
  hidden_size: 768
  num_hidden_layers: 16
  intermediate_size: 3072
  # 64-d heads
  num_attention_heads: 12
  num_key_value_heads: 12
  # MoE
  num_experts: 4                # reduced from 8 to cut total params/memory; keep k=2 to preserve active compute
  num_experts_per_tok: 2
  moe_intermediate_size: 4096   # keep widths to stay near ~0.5B active params per token
  shared_expert_intermediate_size: 4096  # keep shared path width to maintain compute
  decoder_sparse_step: 1        # MoE in every layer

  tie_word_embeddings: true
  qkv_bias: false
  trust_remote_code: false
  source: "huggingface"
name: Qwen2Moe

# Expected (approx):
# - Active params/token ≈ unchanged (~0.5B) for Chinchilla @ 10B tokens
# - Total params ↓ significantly vs 8 experts (optimizer state + checkpoints smaller)
# Fit tips: bf16, gradient checkpointing, use_cache=False, micro-batch 16–64 at T=1024.