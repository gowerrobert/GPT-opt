config:
  # Dimensions chosen to be similar to GPT-small scale while remaining MoE
  vocab_size: 50257            # matches your gpt2-tokenized data
  hidden_size: 256
  intermediate_size: 1024
  num_hidden_layers: 12
  num_attention_heads: 8
  num_key_value_heads: 8
  # Optional: leave trust_remote_code here for parity, unused in custom config path
  trust_remote_code: false
  source: "huggingface"
name: Qwen2Moe