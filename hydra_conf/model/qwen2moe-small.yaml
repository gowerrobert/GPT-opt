config:
  vocab_size: 50257
  hidden_size: 256
  intermediate_size: 1024                 # kept for clarity; unused when every layer is MoE
  num_hidden_layers: 12
  # Use 64-d heads for better attention quality at H=256
  num_attention_heads: 4
  num_key_value_heads: 4
  # MoE: scale to ~124M params
  num_experts: 8                           # from 4 -> 8
  num_experts_per_tok: 2                   # top-k routing
  moe_intermediate_size: 1280              # per-expert FFN width (~5xH)
  shared_expert_intermediate_size: 1536    # shared FFN width (~6xH)
  decoder_sparse_step: 1                   # MoE in every layer
  tie_word_embeddings: true                # saves V*H params
  qkv_bias: true
  trust_remote_code: false
  source: "huggingface"
name: Qwen2Moe

# Params: 124,582,400