# config.yaml

name: "tiny_shakespeare_small"

optimizer_params:
  - name: adam
    lr: [0.0001]
    weight_decay: 0
    lr_schedule: constant

  # - name: momo-adam
  #   lr: [0.1]
  #   weight_decay: 0
  #   lr_schedule: constant

  # - name: sgd-m
  #   lr: [0.001,  0.1, 1]
  #   weight_decay: 0
  #   momentum: 0.9
  #   dampening: 0.9
  #   lr_schedule: warm-up-cosine
  #   warm_up_percent: 0.2

training_params:
  batch_size: 8
  num_epochs: 1
  max_length: 512

gpt_model:
  model_name: "gpt2-medium"  #  You can use one of the pre-defined models of transformers, or you can specify the exact dimension below
  n_embd: 768    # Hidden size used in distilgpt2
  n_layer: 2    # Number of layers in distilgpt2
  n_head: 4    # Number of attention heads in distilgpt2
  vocab_size: 50304
  tokenizer_name: "gpt2-large"

dataset:
  name: "tiny_shakespeare"  