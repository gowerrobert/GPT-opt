# config.yaml
name: "wikitext-2"

optimizer_params:
  - name: momo
    lr: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]
    weight_decay: 0
    beta: [0.9]
    bias_correction: false
    lr_schedule: constant

  - name: adam
    lr: [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]
    weight_decay: 0
    lr_schedule: constant

  - name: momo-adam
    lr: [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 100]
    weight_decay: 0
    lr_schedule: constant

  - name: sgd-m
    lr: [0.001, 0.01, 0.1, 1, 10, 100]
    weight_decay: 0
    momentum: 0.9
    dampening: 0.9
    lr_schedule: warm-up-cosine
    warm_up_percent: 0.2


training_params:
  batch_size: 8
  num_epochs: 1
  max_length: 512
  warm_up_percent: 0.2
  warm_up_peak_mult: 2.0 
  
gpt_model:
  model_name: "gpt2-large"  #  You can use one of the pre-defined models of transformers, or you can specify the exact dimension below
  n_embd: 768    # Hidden size used in distilgpt2
  n_layer: 6    # Number of layers in distilgpt2
  n_head: 12    # Number of attention heads in distilgpt2
  vocab_size: 50304
  tokenizer_name: "gpt2-large"
dataset:
  name: "wikitext-2-raw-v1"