# config.yaml
optimizer_params:
  - name: adam
    lr: [0.00005, 0.0001, 0.0005,  0.005, 0.01, 0.1]
    weight_decay: 0
    lr_schedule: constant

  - name: momo-adam
    lr: [0.00005, 0.0001, 0.0005,  0.005, 0.01, 0.1]
    weight_decay: 0
    lb: 2.5
    lr_schedule: constant

  - name: sgd-m
    lr: [0.00005,  0.0001,  0.0005]
    weight_decay: 0
    momentum: 0.9
    dampening: 0.9
    lr_schedule: warm-up-cosine
    warm_up_percent: 0.2


training_params:
  batch_size: 32
  num_epochs: 1
  max_length: 512
  
gpt_model:
  model_name: "gpt2-large"  #  You can use one of the pre-defined models of transformers, or you can specify the exact dimension below
  tokenizer_name: "gpt2-large"
  
dataset:
  name: "wikitext-2-raw-v1"