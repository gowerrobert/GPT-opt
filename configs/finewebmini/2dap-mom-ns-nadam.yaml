# config.yaml
optimizer_params:
  - name: dap-mom-ns-nadam-0.95-0.999
    lr: [0.0003, 0.0005]
    ema_beta: 0
    dap_beta1: 0.95
    dap_beta2: 0.999
    momentum: 0
    wd: 0
    disable_preconditioning: False
    scalar: False
    ns_pinv_steps: 50
    damping: 0
    nesterov: True
    lr_schedule: constant
    adagradnorm: False
    rcond: 1e-3

training_params:
  tokens_processed: 262144 # 2^18  # 524288 # 2^19
  val_tokens_processed: 8388608 #2^23
  batch_size: 16
  num_epochs: 1
  context_length: 1024
  gradnorm: 1.0
  tensorcore_precision: highest   #Can be highest, high, or medium
  autocast: True
  mixed_precision: bfloat16
  compile: False

logging_params:
  val_tokens_processed: 8388608 #2^23
  log_step: 50
  val_step: 50
  save_ckpt_step: 100
  load_ckpt_step: 0
  keep_last: 2
  ckpt_dir: "" #/mnt/ceph/users/cmodi/gptopt/

gpt_model:
  n_embd: 768    
  n_layer: 12   
  n_head: 12    
  vocab_size: 50257
  flash_attention: True

dataset:
  name: "finewebmini"